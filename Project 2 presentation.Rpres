Modeling User App Rating
========================================================
author: Devin Romines and Joseph Audras
date: May 13, 2019
autosize: true
incremental: true

I. Introduction
========================================================

Google Play Store Data Set

- Contains data on over 10,000 apps
- Looks at user rating, number of installations, and more
- Scraped by Lavanya Gupta, a software enginere at HSBC Software Development
- Last updated 2 months ago

More information about the data set can be found at <https://www.kaggle.com/lava18/google-play-store-apps>.

Our goal with this data is to build two models that can accurately predict the user rating score.

```{r, echo=FALSE, eval=TRUE}
library(tidyverse)
library(lubridate)
library(ggplot2)
```

II. Data Dictionary
========================================================

These are the apps we will be looking at.

- Rating (Numerical), the rating of the app on a scale from 0-5
- Category (Factor), the general type of app, such as Dating, Cooking, or Art and Design
- Reviews (Numerical), the number of reviews that the app received
- Installs (Numerical), the number of installations the app received
- Price (Numerical), the price of the app
- Content.Rating (Factor), the rating for the app, Everyone, Teen, etc.

III. Data Cleaning
========================================================

Part 1: Preparing the Data

For data cleaning, we had to make sure that we converted all variables to the type that they should be, like Installs and Price to numeric, and removing any variables we wouldn't need.

```{r, eval=FALSE}
GooglePlayStore$Price <- gsub("[[:punct:]]", "", GooglePlayStore$Price)
GooglePlayStore$Price <- as.numeric(GooglePlayStore$Price)
GooglePlayStore$Price <- GooglePlayStore$Price/100
```

```{r, echo=FALSE, eval=TRUE}
RawData <- read.csv("googleplaystore.csv")
GooglePlayStore <- RawData[-c(10473), ]
GooglePlayStore <- GooglePlayStore[-grep("NaN", GooglePlayStore$Rating), ]
GooglePlayStore$Reviews <- as.numeric(as.character(GooglePlayStore$Reviews))
GooglePlayStore <- GooglePlayStore[, -5]
GooglePlayStore$Installs <- gsub("\\D", "", GooglePlayStore$Installs)
GooglePlayStore$Installs <- as.numeric(as.character(GooglePlayStore$Installs))
GooglePlayStore <- GooglePlayStore[, -6]
GooglePlayStore$Price <- gsub("[[:punct:]]", "", GooglePlayStore$Price)
GooglePlayStore$Price <- as.numeric(GooglePlayStore$Price)
GooglePlayStore$Price <- GooglePlayStore$Price/100
GooglePlayStore <- GooglePlayStore[, -10]
GooglePlayStore <- GooglePlayStore[, -10]
write.csv(GooglePlayStore,"Prepared_Google_Play_Store_App_Data.csv",row.names=FALSE)
```

III. Data Cleaning
========================================================

Part 2: Splitting the Data

Because we intend on making a predictive model, we split the data into a testing and training data set.

```{r, echo=TRUE, eval=TRUE}
set.seed(42)
GooglePlayStoreTemp <- GooglePlayStore %>% mutate(id=row_number())
Train <- GooglePlayStoreTemp %>% sample_frac(0.6)
Test <- GooglePlayStoreTemp %>% anti_join(Train,by="id")
Train$id <- NULL
Test$id <- NULL
write.csv(Test,"Test.csv",row.names = FALSE)
rm(Test,GooglePlayStoreTemp)
```

IV. Exploratory Data Analysis
========================================================

Part 1: Reviews vs. Rating

<center><font size = "5">
```{r, fig.height=6, fig.width=9}
ggplot(Train) + geom_point(aes(x=log(Reviews),y=Rating), color="blue")
```
</font></center>

IV. Exploratory Data Analysis
========================================================

Part 2: Installs vs. Rating

<center><font size = "5">
```{r, fig.height=6, fig.width=9}
ggplot(Train) + geom_point(aes(x=log(Installs),y=Rating), color="purple")
```
</font></center>

IV. Exploratory Data Analysis
========================================================

Part 3: Price vs. Rating

<center><font size = "5">
```{r, fig.height=6, fig.width=9}
ggplot(Train) + geom_point(aes(x=log(Price),y=Rating), color="red")
```
</font></center>

IV. Exploratory Data Analysis
========================================================

Part 4: Content Rating vs. Rating

<center><font size = "5">
```{r, fig.height=6, fig.width=9}
ggplot(Train) + geom_violin(aes(x=Content.Rating,y=Rating), trim=FALSE, fill="light blue")
```
</font></center>

IV. Exploratory Data Analysis
========================================================

Part 5: Conclusions

- Reviews and Rating are positively related
- Installs and Rating are positively related
- Price and Rating are negatively related
- Content Rating and Rating to not appear to be related

Moving forward, we feel that the models we are aiming to create should include the Review, Installs, and Price variables, possibly Content Rating and Category as well.

V. Broad Questions
========================================================

What makes a good app?

- An app's "good"ness will be determined by the user rating score
- Creating a predictive model will show us if any app characteristics contribute to its "goodness"

Can we know in advance if an app will be good?

- Based on characteristics known before an app is released
- Will be useful for app developers in deciding which apps to support, develop, and market

V. Narrow Questions
========================================================

Can we create a model that accurately predicts an app's user rating based on these variables?

- Number of reviews and installations
- Price and Category

Can we create a model that accurately predicts and app's user rating based purely on variables that would be known before the app is released?

- Content Rating
- Price
- Category

VI. Model Building
========================================================

Part 1: Unconditional Random Forest Model

<center><font size = "4">
```{r, fig.height=5, fig.width=8}
library(randomForest)
set.seed(42)
Model1.A <- randomForest(I(log(Rating)) ~ Reviews + Installs + Price + Category, data = Train, mtry = 4)
Model1.A.Prediction <- predict(Model1.A)
ggplot(Train) + geom_point(aes(x=Model1.A.Prediction,y=log(Rating)),color = "blue") +
  geom_abline(intercept = 0, color = "red")
```
</font></center>

VI. Model Building
========================================================

Part 2: Conditional Random Forest Model

<center><font size = "4">
```{r, fig.height=5, fig.width=8}
set.seed(42)
Model2.A <- randomForest(I(log(Rating)) ~ Price + Category + Content.Rating, data = Train, mtry = 3)
Model2.A.Prediction <- predict(Model2.A)
ggplot(Train) + geom_point(aes(x=Model2.A.Prediction,y=log(Rating)),color = "blue") +
  geom_abline(intercept = 0, color = "red")
```
</font></center>

VI. Model Testing
========================================================

Part 1: Unconditional Model Test

<center><font size = "4">
```{r, fig.height=5, fig.width=8}
Test <- read.csv("Test.csv")
levels(Test$Category) <- levels(Train$Category)
Model1.A.Prediction.Test <- predict(Model1.A,newdata=Test)
ggplot(Test) + geom_point(aes(x=Model1.A.Prediction.Test,y=log(Rating)),color = "green") +
  geom_abline(intercept = 0, color = "red")
```
</font></center>

VI. Model Testing
========================================================

Part 2: Conditional Model Test

<center><font size = "4">
```{r, fig.height=5, fig.width=8}
levels(Test$Category) <- levels(Train$Category)
levels(Test$Content.Rating) <- levels(Train$Content.Rating)
Model2.A.Prediction.Test <- predict(Model2.A,newdata=Test)
ggplot(Test) + geom_point(aes(x=Model2.A.Prediction.Test,y=log(Rating)),color = "green") +
  geom_abline(intercept = 0, color = "red")
```
</font></center>

VII. Results
========================================================

Overall, we generated two models, an unconditional model and a conditional model to predict user rating scores.

- Random Forest Models were better than Linear Models
- Both models were decent with the test data set
- Both models did not do well at predicting the user rating

VIII. Discussion and Conclusions
========================================================

- The models tended to overestimate low user rating scores, probably because most scores were high
- The number of installations and reviews were the most powerful predictors
- Future work should collect more data from different variables to try and find better connections
- From these models, free, highly downloaded, highly reviewed, rated E apps will get the highest user rating
- Our research shows that there is a correlation somewhere, and with more research, perhaps the information could be useful to businesses